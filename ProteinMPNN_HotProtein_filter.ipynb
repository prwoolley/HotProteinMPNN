{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MMSeqs2 Clusters and remove all clusters with MPNN data in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92124,)\n",
      "(72695,)\n"
     ]
    }
   ],
   "source": [
    "clusters_unfiltered = pd.read_csv('./ProteinMPNN_HotProtein_mmseqs2/ProteinMPNN_HotProtein.Res_cluster.tsv',sep='\\t',header=None,names=['Cluster','UniProt'])\n",
    "print(clusters_unfiltered['Cluster'].unique().shape)\n",
    "with open('./ProteinMPNN_Data/ProteinMPNN_TrainingData_Unique_UniProtAccessions.txt') as f:\n",
    "    MPNN_data = pd.Series(f.readlines()).str.rstrip('\\n')\n",
    "\n",
    "clusters_unfiltered['MPNN'] = clusters_unfiltered['UniProt'].isin(MPNN_data).astype(int) # 1 if the UniProt is in MPNN_data, 0 if not\n",
    "clusters = clusters_unfiltered.groupby('Cluster').filter(lambda x: (x['MPNN'] == 0).all()) # Remove all clusters that have any MPNN proteins\n",
    "print(clusters['Cluster'].unique().shape)\n",
    "\n",
    "# 92124 total clusters (MPNN and HotProtein combined)\n",
    "# 72695 clusters with only HotProtein proteins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining clusters, remove observations that don't have an accompanied AF2 structure and add temperature class info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182305, 2)\n",
      "(181914, 2)\n",
      "(72695,)\n",
      "(72531,)\n"
     ]
    }
   ],
   "source": [
    "# Adding temperature class labels\n",
    "with open('S/S_target_classification.pkl', 'rb') as f:\n",
    "    S_target_classification = pkl.load(f)\n",
    "S_target_train_classification = pd.DataFrame({key: S_target_classification[key] for key in ['train_names', 'train_labels']})\n",
    "S_target_test_classification = pd.DataFrame({key: S_target_classification[key] for key in ['test_names', 'test_labels']})\n",
    "S_target_classification_concat = pd.concat([S_target_train_classification.rename(columns={'train_names': 'names', 'train_labels': 'labels'}), \n",
    "                             S_target_test_classification.rename(columns={'test_names': 'names', 'test_labels': 'labels'})])\n",
    "print(S_target_classification_concat.shape)\n",
    "\n",
    "# Saving proteins that have AF2 structures\n",
    "S_target_AF2 = os.listdir(\"/stor/work/Ellington/ProteinMPNN/HotProtein/S/AF2/PDB\")\n",
    "S_target_AF2 = pd.Series(S_target_AF2)\n",
    "S_target_AF2 = S_target_AF2.str.split('-').str[1]\n",
    "S_target_names_not_in_AF2 = S_target_classification_concat[~S_target_classification_concat['names'].isin(S_target_AF2)]['names']\n",
    "S_target_classification_concat = S_target_classification_concat[~S_target_classification_concat['names'].isin(S_target_names_not_in_AF2)]\n",
    "print(S_target_classification_concat.shape)\n",
    "\n",
    "# 182305 total HotProteins\n",
    "# 181914 total HotProteins with a mapped structure\n",
    "\n",
    "print(clusters['Cluster'].unique().shape)\n",
    "clusters = clusters.merge(S_target_classification_concat, left_on='UniProt', right_on='names', how='left')\n",
    "clusters.drop(columns=['names','MPNN'],inplace=True)\n",
    "clusters.dropna(subset=['labels'], inplace=True)\n",
    "print(clusters['Cluster'].unique().shape)\n",
    "\n",
    "# 72695 clusters before filtering\n",
    "# 72531 clusters after filtering\n",
    "\n",
    "\n",
    "### Adding temperature class labels to clusters_unfiltered, for reference\n",
    "clusters_unfiltered = clusters_unfiltered.merge(S_target_classification_concat, left_on='UniProt', right_on='names', how='left')\n",
    "clusters_unfiltered.drop(columns=['names','MPNN'], inplace=True)\n",
    "clusters_unfiltered['labels'].fillna(-1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting one representative UniProt accession per cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each temp range in s2c5:\n",
    "* Make a 80/10/10 train/test/valid split\n",
    "* Load pt file information and create list.csv, train_clusters, test_clusters.txt, valid_clusters.txt\n",
    "* Add ProteinMPNN validation clusters to the valid_clusters.txt to gage how well the new model performs on generic proteins during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5103, 2)\n",
      "(17985, 2)\n",
      "(17321, 2)\n",
      "(31044, 2)\n",
      "(14676, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    temp_range = clusters[clusters['labels'] == i]\n",
    "    temp_range = temp_range.groupby('Cluster')\n",
    "    temp_range = temp_range.first()\n",
    "    temp_range = pd.DataFrame(temp_range.values, columns=temp_range.columns)\n",
    "    print(temp_range.shape)\n",
    "    temp_range = temp_range.sample(frac=1, random_state=1) # This shuffles the rows\n",
    "    temp_range = temp_range.reset_index(drop=True) # Resetting the index\n",
    "    temp_range_train = temp_range.iloc[:int(temp_range.shape[0]*0.8)] # Taking the first 80% of the rows\n",
    "    temp_range_valid = temp_range.iloc[int(temp_range.shape[0]*0.8):] # Taking the last 20% of the rows\n",
    "\n",
    "    hotprotein_list_csv = pd.DataFrame(columns=('CHAINID','DEPOSITION','RESOLUTION','HASH','CLUSTER','SEQUENCE'))\n",
    "    for pdb in temp_range['UniProt']:\n",
    "        pt = torch.load('/stor/work/Ellington/ProteinMPNN/HotProtein/S/PDB_pt/'+pdb+'.pt')\n",
    "        seq = pt['seq'][0][0]\n",
    "        chainID = pt['id']+'_A'\n",
    "        deposition = '2017-02-27'\n",
    "        resolution = 1.0\n",
    "        hash = str(len(hotprotein_list_csv)).zfill(6)\n",
    "        cluster = len(hotprotein_list_csv)\n",
    "        hotprotein_list_csv.loc[len(hotprotein_list_csv)] = [chainID, deposition, resolution, hash, cluster, seq]\n",
    "\n",
    "    hotprotein_list_csv.to_csv('./Models/class_'+str(i)+'/list.csv',sep=',',header=False, index=False)\n",
    "    np.savetxt('./Models/class_'+str(i)+'/train_clusters.txt', temp_range_train.index.values, fmt='%s')\n",
    "    #np.savetxt('./Models/class_'+str(i)+'/test_clusters.txt', test_data, fmt='%s')\n",
    "    np.savetxt('./Models/class_'+str(i)+'/valid_clusters.txt', temp_range_valid.index.values, fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36359, 2)\n",
      "(42727, 2)\n"
     ]
    }
   ],
   "source": [
    "s2c2 = [[0,1,2],[3,4]]\n",
    "for i in range(2):\n",
    "    temp_range = clusters[clusters['labels'].isin(s2c2[i])]\n",
    "    temp_range = temp_range.groupby('Cluster')\n",
    "    temp_range = temp_range.first()\n",
    "    temp_range = pd.DataFrame(temp_range.values, columns=temp_range.columns)\n",
    "    print(temp_range.shape)\n",
    "    temp_range = temp_range.sample(frac=1, random_state=1) # This shuffles the rows\n",
    "    temp_range = temp_range.reset_index(drop=True) # Resetting the index\n",
    "    temp_range_train = temp_range.iloc[:int(temp_range.shape[0]*0.8)] # Taking the first 80% of the rows\n",
    "    temp_range_valid = temp_range.iloc[int(temp_range.shape[0]*0.8):] # Taking the last 20% of the rows\n",
    "\n",
    "    hotprotein_list_csv = pd.DataFrame(columns=('CHAINID','DEPOSITION','RESOLUTION','HASH','CLUSTER','SEQUENCE'))\n",
    "    for pdb in temp_range['UniProt']:\n",
    "        pt = torch.load('/stor/work/Ellington/ProteinMPNN/HotProtein/S/PDB_pt/'+pdb+'.pt')\n",
    "        seq = pt['seq'][0][0]\n",
    "        chainID = pt['id']+'_A'\n",
    "        deposition = '2017-02-27'\n",
    "        resolution = 1.0\n",
    "        hash = str(len(hotprotein_list_csv)).zfill(6)\n",
    "        cluster = len(hotprotein_list_csv)\n",
    "        hotprotein_list_csv.loc[len(hotprotein_list_csv)] = [chainID, deposition, resolution, hash, cluster, seq]\n",
    "\n",
    "    hotprotein_list_csv.to_csv('./Models/class_'+'_'.join(map(str, s2c2[i]))+'/list.csv',sep=',',header=False, index=False)\n",
    "    np.savetxt('./Models/class_'+'_'.join(map(str, s2c2[i]))+'/train_clusters.txt', temp_range_train.index.values, fmt='%s')\n",
    "    #np.savetxt('./Models/class_'+'_'.join(map(str, s2c2[i]))+'/test_clusters.txt', test_data, fmt='%s')\n",
    "    np.savetxt('./Models/class_'+'_'.join(map(str, s2c2[i]))+'/valid_clusters.txt', temp_range_valid.index.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
